{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with PhenoBench \n",
    "\n",
    "In this short tutorial, we will show you the main parts of the dataset. We assume that you have downloaded the PhenoBench dataset from the [dataset website](https://www.phenobench.org/dataset.html) and placed it in your home directory at `~/data/PhenoBench`; this folder contains the train, val, and test set of the dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "For installing the devkit, you simply have to type:\n",
    " \n",
    " ```pip install phenobench```\n",
    " \n",
    " This command will install the needed dependencies and provide a simple dataset class, which could also be used to implement your own PyTorch, Tensorflow, or other custom dataloader -- it can also be the basis for your very custom processing pipeline."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "In the PhenoBench devkit, we have the dataset class called `PhenoBench`, which provides functionally to read the data from a given `root` directory. Here, `root` corresponds to the aforementioned dataset directory, i.e., `~/data/PhenoBench`, which contains the following directories:\n",
    "\n",
    "<pre>\n",
    "PhenoBench\n",
    "├── test\n",
    "│   └── images\n",
    "├── train\n",
    "│   ├── images\n",
    "│   ├── leaf_instances\n",
    "│   ├── leaf_visibility\n",
    "│   ├── plant_instances\n",
    "│   ├── plant_visibility\n",
    "│   └── semantics\n",
    "└── val\n",
    "    ├── images\n",
    "    ├── leaf_instances\n",
    "    ├── leaf_visibility\n",
    "    ├── plant_instances\n",
    "    ├── plant_visibility\n",
    "    └── semantics\n",
    "</pre>\n",
    "\n",
    "Note that only train and val contain annotations for the specific `target_types`:\n",
    "\n",
    "- **semantics**: pixel-wise semantics as integers, such that `1` corresponds to crop, `2` corresponds to `weed` with additional partial labels with class id `3` for partial crops and `4` for partial weeds. Due to the annotation process, where we label complete plants, we can determine if the area of plant pixels in a cropped image is below 50% and mark these areas as partially visible.\n",
    "- **plant_instances**: pixel-wise instance id as integers. With `make_unique_ids = True` the dataloader will remap all instances to the range `[1, N + M]`, where `N` is the number of crop plants and `M` is the number of weed instances. Without `make_unique_id`, the ids correspond to the arbitrary instance id of the global image. \n",
    "- **leaf_instances**: pixel-wise instance id as integers. With `make_unique_ids = True` the dataloader will remap all instances to the range `[1, N + M]`, where `N` is the number of crop plants and `M` is the number of weed instances. Without `make_unique_id`, the ids correspond to the arbitrary instance id of the global image. \n",
    "- **plant_visibility**: pixel-wise visibility mask, where visibility is given in range `[0,1]` encoding the percentage of pixels visible in the image.\n",
    "- **leaf_visibility**: pixel-wise visibility mask, where visibility is given in range `[0,1]` encoding the percentage of pixels visible in the image.\n",
    "\n",
    "Additionally, there are \"meta-targets\" that are generated from the pixel-wise instance annotation:\n",
    "\n",
    "- **plant_bboxes**: plant bounding boxes represented as list of tuples containing `{\"label\", \"center\", \"width\", \"height\", \"corner\"}`, where `center` corresponds to the center of the bounding box, `width` and `height` refer tot he width/height of the bounding box, and `corner` to the corner coordinate of the upper-left corner.\n",
    "- **leaf_bboxes**: leaf bounding boxes represented as list of tuples containing `{\"label\", \"center\", \"width\", \"height\", \"corner\"}`, where `center` corresponds to the center of the bounding box, `width` and `height` refer tot he width/height of the bounding box, and `corner` to the corner coordinate of the upper-left corner.\n",
    "\n",
    "Enough theory ... Let's actually visualize some example images and annotations from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phenobench import PhenoBench\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "train_data = PhenoBench(\"~/data/PhenoBench\", \n",
    "                        target_types=[\"semantics\", \"plant_instances\", \"leaf_instances\", \"plant_bboxes\", \"leaf_bboxes\"])\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"PhenoBench ({train_data.split} split) contains {len(train_data)} images. We loaded the following targets: {train_data.target_types}.\"\n",
    ")\n",
    "print(\"The first entry contains the following fields:\")\n",
    "pprint([f\"{k} -> {type(v)}\" for k, v in train_data[0].items()])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Data\n",
    "\n",
    "Besides the generic loader of the dataset, we also provide some visualization functions that can be used in combination with matplotlib. These drawing functions can be used to visualize the dataset, but also to render predictions.\n",
    "\n",
    "All drawing functions are available in the package `phenobench.visualization` and here we show some examples generated from the dataset itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from phenobench.visualization import draw_semantics, draw_instances, draw_bboxes\n",
    "\n",
    "\n",
    "n_samples = 4\n",
    "n_rows = 4\n",
    "fig, axes = plt.subplots(ncols=n_samples, nrows=n_rows, figsize=(3 * n_samples, 3 * n_rows))\n",
    "\n",
    "indexes = np.random.choice(len(train_data), n_samples)\n",
    "\n",
    "for i in range(n_rows):\n",
    "    for j in range(n_samples):\n",
    "        axes[i, j].set_axis_off()\n",
    "\n",
    "for id, idx in enumerate(indexes):\n",
    "    axes[0, id].set_title(os.path.splitext(train_data[idx][\"image_name\"])[0])\n",
    "\n",
    "    draw_semantics(axes[0, id], train_data[idx][\"image\"], train_data[idx][\"semantics\"], alpha=0.5)\n",
    "    draw_instances(axes[1, id], train_data[idx][\"image\"], train_data[idx][\"plant_instances\"], alpha=0.5)\n",
    "    draw_instances(axes[2, id], train_data[idx][\"image\"], train_data[idx][\"leaf_instances\"], alpha=0.5)\n",
    "    draw_bboxes(axes[3, id], train_data[idx][\"image\"], train_data[idx][\"plant_bboxes\"])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "If you have issues with the usage of the data, but also want to provide feedback regarding the functionality, feel free to open an issues or write us an email.\n",
    "\n",
    "\n",
    "Good luck with the training of your models and we are looking forward to the amazing things you will do with our data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
